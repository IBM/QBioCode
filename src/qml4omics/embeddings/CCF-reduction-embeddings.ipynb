{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T16:40:45.831433Z",
     "iopub.status.busy": "2024-07-22T16:40:45.831150Z",
     "iopub.status.idle": "2024-07-22T16:40:45.839706Z",
     "shell.execute_reply": "2024-07-22T16:40:45.839202Z",
     "shell.execute_reply.started": "2024-07-22T16:40:45.831413Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndimage\n",
    "# from proc_utils import (protocol_table, read_dicom_stack, set_slice, get_slice, get_series_tag)\n",
    "# from relaxometry import nlsq_fitting\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, BatchNormalization, Activation, concatenate\n",
    "\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError, MeanSquaredError, MeanAbsolutePercentageError, CosineSimilarity\n",
    "\n",
    "from tensorly.decomposition import parafac\n",
    "import tensorly as tl\n",
    "\n",
    "# random noise generation reproducibility\n",
    "np.random.seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T16:40:45.848644Z",
     "iopub.status.busy": "2024-07-22T16:40:45.848481Z",
     "iopub.status.idle": "2024-07-22T16:40:45.857578Z",
     "shell.execute_reply": "2024-07-22T16:40:45.857164Z",
     "shell.execute_reply.started": "2024-07-22T16:40:45.848629Z"
    }
   },
   "outputs": [],
   "source": [
    "parent_dir = '/dccstor/fmm/users/mcburch/workspaces/scratch_dir/'\n",
    "# parent_dir = ''\n",
    "input_dir = parent_dir + \"OAI/00m/0.C.2/9000296\"     # input directory with DICOM data\n",
    "output_dir = parent_dir + \"OAI/imgs/9000296\"         # output directory to save 3D volume data\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T16:40:45.858172Z",
     "iopub.status.busy": "2024-07-22T16:40:45.858013Z",
     "iopub.status.idle": "2024-07-22T16:40:45.869379Z",
     "shell.execute_reply": "2024-07-22T16:40:45.868967Z",
     "shell.execute_reply.started": "2024-07-22T16:40:45.858156Z"
    }
   },
   "outputs": [],
   "source": [
    "# get protocol table\n",
    "data_sheet = f\"{output_dir}/protocol_table.csv\"\n",
    "\n",
    "# # create protocol table\n",
    "# df = protocol_table(input_dir, relative=True)\n",
    "# df.to_csv(data_sheet, index=False)\n",
    "\n",
    "if not os.path.isfile(data_sheet):\n",
    "    # create protocol table\n",
    "    df = protocol_table(input_dir, relative=True)\n",
    "    df.to_csv(data_sheet, index=False)\n",
    "else:\n",
    "    # read protocol table\n",
    "    df = pd.read_csv(data_sheet)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T16:40:45.870564Z",
     "iopub.status.busy": "2024-07-22T16:40:45.870398Z",
     "iopub.status.idle": "2024-07-22T16:40:46.418836Z",
     "shell.execute_reply": "2024-07-22T16:40:46.418279Z",
     "shell.execute_reply.started": "2024-07-22T16:40:45.870548Z"
    }
   },
   "outputs": [],
   "source": [
    "# create SAG_T2_MAP_RIGHT volume\n",
    "\n",
    "seq = \"SAG_T2_MAP_RIGHT\"\n",
    "# seq = \"SAG_3D_DESS_RIGHT\"\n",
    "\n",
    "img_dir =  df.dir[df.SeriesDescription == seq].values[0]\n",
    "acquisition_type =  df.MRAcquisitionType[df.SeriesDescription == seq].values[0].lower()\n",
    "encoding_direction =  df.InPlanePhaseEncodingDirection[df.SeriesDescription == seq].values[0]\n",
    "\n",
    "_, descriptions = read_dicom_stack(img_dir, outdir=output_dir)\n",
    "encoding_axes_dict = {'ROW':0, 'COL':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dir[df.SeriesDescription == seq].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T16:40:46.419722Z",
     "iopub.status.busy": "2024-07-22T16:40:46.419446Z",
     "iopub.status.idle": "2024-07-22T16:40:46.427455Z",
     "shell.execute_reply": "2024-07-22T16:40:46.427038Z",
     "shell.execute_reply.started": "2024-07-22T16:40:46.419702Z"
    }
   },
   "outputs": [],
   "source": [
    "# read volume data\n",
    "nii_file = f\"{output_dir}/{descriptions[0]}.nii.gz\"\n",
    "nimg = nib.load(nii_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T16:40:46.428105Z",
     "iopub.status.busy": "2024-07-22T16:40:46.427947Z",
     "iopub.status.idle": "2024-07-22T16:40:47.198256Z",
     "shell.execute_reply": "2024-07-22T16:40:47.197729Z",
     "shell.execute_reply.started": "2024-07-22T16:40:46.428089Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert to numpy array\n",
    "img_orig = nimg.get_fdata()   # float data\n",
    "shape_orig = img_orig.shape\n",
    "\n",
    "if acquisition_type == '2d':\n",
    "    # subsample each 2d slice acquired\n",
    "    img_axis = 2\n",
    "\n",
    "    if img_orig.ndim == 4:\n",
    "        shape_new = *shape_orig[:2], shape_orig[2]*shape_orig[3]\n",
    "        img_acq = np.reshape(img_orig, shape_new)\n",
    "    elif len(shape_orig) == 3:\n",
    "        img_acq = img_orig.copy()\n",
    "        shape_new = shape_orig[:]        \n",
    "    \n",
    "elif acquisition_type == '3d':\n",
    "    # subsample each 3d volume acquired\n",
    "    img_axis = 3\n",
    "    \n",
    "    if img_orig.ndim == 3: \n",
    "        shape_new = *shape_orig, 1\n",
    "        img_acq = np.reshape(img_orig, shape_new)\n",
    "    else:\n",
    "        img_acq = img_orig.copy()\n",
    "        shape_new = shape_orig[:]\n",
    "\n",
    "num_images = shape_new[img_axis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T16:40:47.199064Z",
     "iopub.status.busy": "2024-07-22T16:40:47.198875Z",
     "iopub.status.idle": "2024-07-22T16:40:47.204844Z",
     "shell.execute_reply": "2024-07-22T16:40:47.204407Z",
     "shell.execute_reply.started": "2024-07-22T16:40:47.199047Z"
    }
   },
   "outputs": [],
   "source": [
    "# define subsampling factor \n",
    "# if less than 1 then image will be subsampled\n",
    "sampling_factor = 0.25  \n",
    "\n",
    "# define noise addition factor\n",
    "# standard deviation multiplied by average value of the nominal absolute value of Fourier spectrum. think of it as 1/SNR\n",
    "noise_factor = 0.05     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T16:40:47.205475Z",
     "iopub.status.busy": "2024-07-22T16:40:47.205315Z",
     "iopub.status.idle": "2024-07-22T16:40:52.124733Z",
     "shell.execute_reply": "2024-07-22T16:40:52.124194Z",
     "shell.execute_reply.started": "2024-07-22T16:40:47.205459Z"
    }
   },
   "outputs": [],
   "source": [
    "# loop through acquired data to fft then shift to center\n",
    "ft_acq = [np.fft.fftshift(np.fft.fftn(img_acq[...,i])) for i in range(num_images)]\n",
    "ft_acq = np.stack(ft_acq, axis=-1)\n",
    "\n",
    "# get number of phase encode lines\n",
    "phase_endoding_direction = encoding_axes_dict[encoding_direction] # ROW:0, COL:1\n",
    "phase_encoding_steps = ft_acq.shape[phase_endoding_direction]\n",
    "\n",
    "# compute the relative noise level to add (1/SNR)\n",
    "noise_std = noise_factor * np.mean(np.abs(ft_acq))\n",
    "\n",
    "# sample the noise in the Fourier domain (Gaussian complex noise)\n",
    "noise_spectrum = noise_std*np.random.randn(*ft_acq.shape) + 1j*noise_std*np.random.randn(*ft_acq.shape) \n",
    "ft_noisy = ft_acq + noise_spectrum\n",
    "\n",
    "# phase encode lines to keep\n",
    "ft_mask = np.zeros_like(ft_acq)\n",
    "phase_encoding_lines = np.arange(np.round(phase_encoding_steps*(1-sampling_factor)/2), np.round(phase_encoding_steps*(1+sampling_factor)/2))\n",
    "phase_encoding_lines = phase_encoding_lines.astype('int')\n",
    "\n",
    "# crop according to InPlanePhaseEncodingDirection [ROW/COL]\n",
    "ft_mask = set_slice(ft_mask, phase_endoding_direction, phase_encoding_lines, 1)\n",
    "\n",
    "# simulate low resolution acquisition i.e. crop out the higher resolution samples in Fourier domain\n",
    "ft_subsampled = ft_noisy * ft_mask\n",
    "\n",
    "# # zero padding interpolates the image to the original input size. However this can lead to ringing artifacts depending on the sampling of the original data\n",
    "# ft_subsampled = get_slice(ft_subsampled, phase_endoding_direction, phase_encoding_lines)\n",
    "\n",
    "# shift back, compute inverse Fourier transform\n",
    "img_subsampled = [np.abs(np.fft.ifftn(np.fft.ifftshift(ft_subsampled[...,i]))) for i in range(num_images)]\n",
    "img_subsampled = np.stack(img_subsampled, axis=-1)\n",
    "img_subsampled = np.reshape(img_subsampled, shape_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T16:40:52.125545Z",
     "iopub.status.busy": "2024-07-22T16:40:52.125360Z",
     "iopub.status.idle": "2024-07-22T16:40:52.895981Z",
     "shell.execute_reply": "2024-07-22T16:40:52.895447Z",
     "shell.execute_reply.started": "2024-07-22T16:40:52.125527Z"
    }
   },
   "outputs": [],
   "source": [
    "# resizing as needed\n",
    "resize_factor = [1] * img_subsampled.ndim   # scale factor for each dimension. \n",
    "resize_factor[:2] = [0.5]*2                 # We reduce the InPlaneResolution by half\n",
    "    \n",
    "img_interp = ndimage.interpolation.zoom(img_subsampled,resize_factor, order=1) # reduce resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### visualize downsampled echoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T16:40:52.896831Z",
     "iopub.status.busy": "2024-07-22T16:40:52.896642Z",
     "iopub.status.idle": "2024-07-22T16:40:52.902441Z",
     "shell.execute_reply": "2024-07-22T16:40:52.902020Z",
     "shell.execute_reply.started": "2024-07-22T16:40:52.896814Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T16:40:52.903069Z",
     "iopub.status.busy": "2024-07-22T16:40:52.902915Z",
     "iopub.status.idle": "2024-07-22T16:40:54.171497Z",
     "shell.execute_reply": "2024-07-22T16:40:54.171008Z",
     "shell.execute_reply.started": "2024-07-22T16:40:52.903053Z"
    }
   },
   "outputs": [],
   "source": [
    "# MESE signal decay with increasing echos\n",
    "slice_num = shape_orig[2]//2 # select a slice to visualize\n",
    "\n",
    "num_volumes = 1 if img_orig.ndim == 3 else shape_orig[-1]\n",
    "\n",
    "num_volumes = 1 # hard coding just to experiment with 1 echo\n",
    "\n",
    "vmin = 0\n",
    "vmax = 0.5 *img_orig.max()\n",
    "\n",
    "ncol = 3\n",
    "fig, ax = plt.subplots(num_volumes, ncol, figsize=(10, 20))\n",
    "axs = ax.flat\n",
    "fig_props = {'cmap':'gray', 'vmin':vmin, 'vmax':vmax}\n",
    "\n",
    "# iterate through each echo 'v' \n",
    "for v in range(num_volumes):\n",
    "    ai = ncol * v\n",
    "    axs[ai].imshow(img_orig[...,slice_num, v].T, **fig_props)\n",
    "    axs[ai].set_title(f\"original: Echo {v}\")\n",
    "\n",
    "    axs[ai+1].imshow(img_subsampled[...,slice_num, v].T, **fig_props)\n",
    "    axs[ai+1].set_title(f\"subsampled: Echo {v}\")\n",
    "\n",
    "    axs[ai+2].imshow(img_interp[...,slice_num, v].T, **fig_props)\n",
    "    axs[ai+2].set_title(f\"resized: Echo {v}\")\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lower dimensional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rearranged_img_orig= np.transpose(img_orig, (2,3,0,1))\n",
    "rearranged_img_subsampled = np.transpose(img_subsampled, (2,3,0,1))\n",
    "rearranged_img_interp = np.transpose(img_interp, (2,3,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rearranged_img_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# The input is now treated as having a shape (batch_size, 7, 384, 384), where 7 is the channel dimension\n",
    "# Channel dimension usually being the RGB but in this case echo instensity \n",
    "\n",
    "# Assuming the data is a PyTorch tensor of shape (batch_size, 7, 384, 384)\n",
    "# data = torch.randn((28, 7, 384, 384))  # Example data with batch size 28\n",
    "tmp = rearranged_img_orig.astype(np.float32)\n",
    "data = torch.from_numpy(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Autoencoder Model\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(7, 64, kernel_size=3, stride=2, padding=1),  # (64, 192, 192)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # (128, 96, 96)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # (256, 48, 48)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),  # (512, 24, 24)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 7, kernel_size=3, stride=2, padding=1),    # (7, 16, 16)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(7, 512, kernel_size=3, stride=2, padding=1, output_padding=1),  # (512, 24, 24)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),  # (256, 48, 48)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),  # (128, 96, 96)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),   # (64, 192, 192)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 7, kernel_size=3, stride=2, padding=1, output_padding=1),     # (7, 384, 384)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return latent, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # attempting generalized version (WORKS BUT DOESN'T IMPROVE MSE)\n",
    "\n",
    "# class ConvAutoencoder(nn.Module):\n",
    "#     def __init__(self, input_channels=7, target_channels=7, num_layers=5, initial_filters=64):\n",
    "#         super(ConvAutoencoder, self).__init__()\n",
    "\n",
    "#         # Calculate downsampling factor based on num_layers\n",
    "#         target_size = 16  # target spatial dimensions\n",
    "#         input_size = 384  # initial spatial dimensions\n",
    "#         downsample_factor = int((input_size // target_size) ** (1 / num_layers))\n",
    "\n",
    "#         # Encoder\n",
    "#         encoder_layers = []\n",
    "#         in_channels = input_channels\n",
    "#         out_channels = initial_filters\n",
    "\n",
    "#         for _ in range(num_layers):\n",
    "#             encoder_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1))\n",
    "#             encoder_layers.append(nn.ReLU())\n",
    "#             in_channels = out_channels\n",
    "#             out_channels = min(out_channels * 2, 512)  # Double filters each layer, capped at 512\n",
    "\n",
    "#         encoder_layers.append(nn.Conv2d(in_channels, target_channels, kernel_size=3, stride=2, padding=1))\n",
    "#         encoder_layers.append(nn.ReLU())\n",
    "#         self.encoder = nn.Sequential(*encoder_layers)\n",
    "\n",
    "#         # Decoder\n",
    "#         decoder_layers = []\n",
    "#         in_channels = target_channels\n",
    "#         out_channels = min(512, initial_filters * (2 ** (num_layers - 1)))\n",
    "\n",
    "#         for _ in range(num_layers):\n",
    "#             decoder_layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "#             decoder_layers.append(nn.ReLU())\n",
    "#             in_channels = out_channels\n",
    "#             out_channels = max(out_channels // 2, initial_filters)  # Halve filters each layer, minimum at initial_filters\n",
    "\n",
    "#         decoder_layers.append(nn.ConvTranspose2d(in_channels, input_channels, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "#         decoder_layers.append(nn.Sigmoid())\n",
    "#         self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         latent = self.encoder(x)\n",
    "#         reconstructed = self.decoder(latent)\n",
    "#         return latent, reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = ConvAutoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    latent, reconstructed = model(data)\n",
    "    loss = criterion(reconstructed, data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings\n",
    "with torch.no_grad():\n",
    "    latent, _ = model(data)\n",
    "\n",
    "# Evaluate the embedding quality\n",
    "reconstruction_error = criterion(reconstructed, data).item()\n",
    "print(f\"Reconstruction Error: {reconstruction_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
