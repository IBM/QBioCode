{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin generation of yamls for manual grid search using QML methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QMethods\n",
    "qmethods = [ 'qnn', 'vqc', 'qsvc' ]\n",
    "reps = [1,2]\n",
    "optimizers = ['COBYLA', 'SPSA' ]\n",
    "entanglements = ['linear', 'full' ]\n",
    "feature_maps = ['Z', 'ZZ']\n",
    "ansatz_types = ['amp', 'esu2']\n",
    "n_components = [5,10]\n",
    "Cs =  [0.1, 1, 10]\n",
    "max_iters = [100,500]\n",
    "embeddings = ['pca', 'nmf', 'none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "dir_home= re.sub( 'qml4omics.*', 'qml4omics', os.getcwd() )\n",
    "dir_config = os.path.join( dir_home, 'configs' )\n",
    "file_template_config = os.path.join( dir_config, 'config.yaml' )\n",
    "dir_config_new = os.path.join( dir_config, 'configs_qml_gridsearch' )\n",
    "used_data_files = os.path.join( dir_config_new, 'used_data_files.csv' )\n",
    "\n",
    "\n",
    "if not os.path.exists(dir_config_new):\n",
    "    os.mkdir(dir_config_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     method  reps local_optimizer entanglement feature_map ansatz_type  \\\n",
      "0       qnn     1          COBYLA       linear           Z         amp   \n",
      "1       qnn     1          COBYLA       linear           Z         amp   \n",
      "2       qnn     1          COBYLA       linear           Z         amp   \n",
      "3       qnn     1          COBYLA       linear           Z         amp   \n",
      "4       qnn     1          COBYLA       linear           Z         amp   \n",
      "...     ...   ...             ...          ...         ...         ...   \n",
      "2527   qsvc     1          COBYLA         full          ZZ         amp   \n",
      "2528   qsvc     1          COBYLA         full          ZZ         amp   \n",
      "2532   qsvc     1          COBYLA         full          ZZ         amp   \n",
      "2533   qsvc     1          COBYLA         full          ZZ         amp   \n",
      "2534   qsvc     1          COBYLA         full          ZZ         amp   \n",
      "\n",
      "      n_components     C  max_iter embedding  \n",
      "0                5   1.0       100       pca  \n",
      "1                5   1.0       100       nmf  \n",
      "2                5   1.0       100      none  \n",
      "3                5   1.0       500       pca  \n",
      "4                5   1.0       500       nmf  \n",
      "...            ...   ...       ...       ...  \n",
      "2527             5   1.0       100       nmf  \n",
      "2528             5   1.0       100      none  \n",
      "2532             5  10.0       100       pca  \n",
      "2533             5  10.0       100       nmf  \n",
      "2534             5  10.0       100      none  \n",
      "\n",
      "[324 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#############    \n",
    "import itertools\n",
    "p = [qmethods, reps, optimizers, entanglements, feature_maps, ansatz_types, n_components, Cs, max_iters, embeddings]\n",
    "\n",
    "p_c = pd.DataFrame(list(itertools.product(*p)),\n",
    "                   columns=[\n",
    "                       'method',\n",
    "                       'reps',\n",
    "                       'local_optimizer',\n",
    "                       'entanglement',\n",
    "                       'feature_map',\n",
    "                       'ansatz_type',\n",
    "                       'n_components',\n",
    "                       'C',\n",
    "                       'max_iter',\n",
    "                       'embedding'\n",
    "                   ])\n",
    "p_c.loc[ p_c['method'].isin( ['qnn','vqc']), 'C' ] = 1\n",
    "p_c.loc[ p_c['method'].isin( ['qsvc']), 'ansatz_type' ] = 'amp'\n",
    "p_c.loc[ p_c['method'].isin( ['qsvc']), 'max_iter' ] = 100\n",
    "p_c.loc[ p_c['method'].isin( ['qsvc']), 'local_optimizer' ] = 'COBYLA'\n",
    "\n",
    "p_c = p_c.drop_duplicates()\n",
    "p_c = p_c[~((p_c['n_components'] >= 10) & (p_c['max_iter'] < 500))]\n",
    "p_c = p_c[~((p_c['reps'] > 1) & (p_c['n_components'] <= 10))] # for small feature space, you may not need more than 1 layer of the ansatz\n",
    "print(p_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "864"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############\n",
    "idx = 1 \n",
    "cfg_orig = yaml.safe_load( open( file_template_config, 'r+' ) )\n",
    "\n",
    "if os.path.exists( used_data_files ):\n",
    "    used_files = pd.read_csv(used_data_files)\n",
    "else:\n",
    "    used_files = []\n",
    "\n",
    "# Data Files\n",
    "for dir_data in [\n",
    "    #os.path.join( dir_home, 'data', 'tutorial_test_data', 'higher_dim_datasets'),\n",
    "    os.path.join( dir_home, 'data', 'tutorial_test_data', 'lower_dim_datasets')\n",
    "\n",
    "]:\n",
    "    files = [ fl for fl in os.listdir(dir_data) if 'csv' in fl ]\n",
    "    files.sort()\n",
    "    # remove files previously used    \n",
    "    files = list(set(files).difference(set(used_files)) )\n",
    "\n",
    "    \n",
    "    # Use all files\n",
    "    files = list(np.random.choice( files, int(len(files)*1)))\n",
    "    # Or use a random sample of 10% of files\n",
    "    # files = list(np.random.choice( files, int(len(files)*1)))\n",
    "    used_files = used_files + files\n",
    "    p_c_t = p_c.copy()\n",
    "    if ('moons' in dir_data) | ('circles' in dir_data):\n",
    "        p_c_t = p_c_t[ p_c_t['embedding'] == 'none' ]\n",
    "    else:\n",
    "        p_c_t = p_c_t[ p_c_t['embedding'] != 'none' ]\n",
    "\n",
    "    count = 1\n",
    "    for ix, row in p_c_t.iterrows():\n",
    "        for fl in files:\n",
    "            file_yaml = os.path.join( dir_config_new, 'exp_' + str(idx) + '.yaml')\n",
    "            key = row['method'] + '_' + re.sub( '.csv', '', fl )\n",
    "            cfg = cfg_orig.copy()\n",
    "            cfg['yaml'] = file_yaml\n",
    "            cfg['model'] = [row['method']]\n",
    "            cfg['file_dataset'] = str(fl)\n",
    "            cfg['folder_path'] = re.sub( 'data/', '', dir_data )\n",
    "            cfg['hydra']['run']['dir'] = os.path.join( 'results', 'qmlgridsearch_' + key )\n",
    "            \n",
    "            ###\n",
    "            # This block looks at the input dataset and ensures the embedding method placed in the yaml\n",
    "            # is 'none' if n_components is equal to or greater than the original number of features \n",
    "            # in the input data set.  This should help avoid redundant yaml files, since using either of \n",
    "            # the embedding methods (pca, nmf) in this scenario is essentially the same as choosing 'none', thus\n",
    "            # leading to the same experiment being run, even though the yaml file might be different. \n",
    "            fl_df=pd.read_csv(dir_data+'/'+fl)\n",
    "            orig_features = fl_df.shape[1]-1\n",
    "            if row['n_components'] >= orig_features:\n",
    "                #print(orig_features, row['n_components'])\n",
    "                cfg['embeddings'] = [str('none')]\n",
    "                count += 1\n",
    "            else:\n",
    "                cfg['embeddings'] = [row['embedding']]\n",
    "            \n",
    "            cfg['n_components'] = row['n_components']\n",
    "            ###\n",
    "            \n",
    "            cfg[row['method']+'_args']['reps'] = row['reps']\n",
    "            cfg[row['method']+'_args']['entanglement'] = row['entanglement']\n",
    "            cfg[row['method']+'_args']['encoding'] = row['feature_map']\n",
    "            \n",
    "            if row['method'] != 'qsvc':\n",
    "                cfg[row['method']+'_args']['ansatz_type'] = row['ansatz_type']\n",
    "                cfg[row['method']+'_args']['maxiter'] = row['max_iter']\n",
    "            else:\n",
    "                cfg[row['method']+'_args']['C'] = row['C']\n",
    "                cfg[row['method']+'_args']['local_optimizer'] = row['local_optimizer']\n",
    "                            \n",
    "            yaml.dump( cfg, open( file_yaml, 'w'), default_flow_style= False )   \n",
    "            idx += 1\n",
    "\n",
    "pd.Series(used_files).to_csv( used_data_files, index = False)\n",
    "\n",
    "idx-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End generation of yamls for manual grid search using QML methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qml4omics-testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
